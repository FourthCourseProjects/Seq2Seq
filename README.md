# Seq2Seq Models in NLP with PyTorch

[![Skill Icons](https://skillicons.dev/icons?i=py,pytorch&perline=2)](https://skillicons.dev)

This repository hosts a collection of Jupyter notebooks dedicated to exploring the Seq2Seq models, particularly focusing on applications in Natural Language Processing (NLP). The notebooks cover from basic implementations of Seq2Seq models to more advanced concepts like the attention mechanism.

## Table of Contents

- [Introduction](#introduction)
- [Model Overview](#model-overview)
- [Attention Mechanism](#attention-mechanism)
- [Implementation](#implementation)
- [Evaluation](#evaluation)
- [Dependencies](#dependencies)
- [How to Run](#how-to-run)

## Introduction

Seq2Seq models have become a fundamental part of modern NLP. This repository provides a practical approach to understanding these models through hands-on Jupyter notebooks.

## Model Overview

The notebooks start with the basics of Seq2Seq models, explaining their architecture and role in NLP tasks such as machine translation and text summarization.

## Attention Mechanism

Further, we delve into the attention mechanism, an essential improvement in Seq2Seq models that helps in better understanding and translating longer sentences.

## Evaluation

- Includes methods for evaluating the performance of the models.
- Visualizations are provided to understand the efficiency and accuracy of the models.

## How to Run

1. Clone the repository.
2. Install the necessary dependencies mentioned in the `requirements.txt` file.
3. Navigate through the notebooks and run them in an order that suits your learning pace.

---
## Contact

(c) 2023 José Juan Hernández Gálvez 
<br>Github: https://github.com/josejuanhernandezgalvez <br>
(c) 2023 Jorge Lang-Lenton Ferreiro          
Github: https://github.com/JorgeLLF
